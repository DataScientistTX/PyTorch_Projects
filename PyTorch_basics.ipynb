{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPXPHaDamNUQXtBPuWqqRDr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DataScientistTX/PyTorch_Projects/blob/main/PyTorch_basics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aJEfxx7nwfF-",
        "outputId": "5c587d93-a913-4e80-b2c2-5a8037d3b181"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Oct 28 02:17:36 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   60C    P8    10W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "#Checking the GPU from Google Colab\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "G-VXrvxP8pac"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from tqdm.autonotebook import tqdm\n",
        "import pandas as pd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ga76Tupswkrn",
        "outputId": "8868a737-f643-4670-b0e0-e194bff1a3ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-93e2626d8a4a>:4: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
            "  from tqdm.autonotebook import tqdm\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To use PyTorch, we need to import it as the torch package. With it, we can immediately start creating tensors. Every time you nest a list within another list, you create a new dimension of the tensor that PyTorch will produce:"
      ],
      "metadata": {
        "id": "vSNXnsPz8teY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch_scalar = torch.tensor(3.14)\n",
        "torch_vector = torch.tensor([1, 2, 3, 4])\n",
        "torch_matrix = torch.tensor([[1, 2,],\n",
        "                             [3, 4,],\n",
        "                             [5, 6,],\n",
        "                             [7, 8,]])\n",
        "torch_tensor3d = torch.tensor([\n",
        "                            [\n",
        "                            [ 1,  2,  3],\n",
        "                            [ 4,  5,  6],\n",
        "                            ],\n",
        "                            [\n",
        "                            [ 7,  8,  9],\n",
        "                            [10, 11, 12],\n",
        "                            ],\n",
        "                            [\n",
        "                            [13, 14, 15],\n",
        "                            [16, 17, 18],\n",
        "                            ],\n",
        "                            [\n",
        "                            [19, 20, 21],\n",
        "                            [22, 23, 24],\n",
        "                            ]\n",
        "])"
      ],
      "metadata": {
        "id": "Y8gyfZTOzTS9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we print the shapes of these tensors, you should see the same shapes shown earlier. Again, while scalars, vectors, and matrices are different things, they are unified under the larger umbrella of tensors. We care about this because we use tensors of different shapes to represent different types of data. We get to those details later; for now, we focus on the mechanics PyTorch provides to work with tensors:"
      ],
      "metadata": {
        "id": "Eem6-HkR9ncB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch_scalar.shape)\n",
        "print(torch_vector.shape)\n",
        "print(torch_matrix.shape)\n",
        "print(torch_tensor3d.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xn0vb2G_5ixm",
        "outputId": "8870d942-f85c-4370-95ab-af28aa6b54ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([])\n",
            "torch.Size([4])\n",
            "torch.Size([4, 2])\n",
            "torch.Size([4, 2, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you have done any ML or scientific computing in Python, you have probably used the NumPy library. As you would expect, PyTorch supports converting NumPy objects into their PyTorch counterparts. Since both of them represent data as tensors, this is a painless process. The following two code blocks show how we can create a random matrix in NumPy and then convert it into a PyTorch Tensor object:\n"
      ],
      "metadata": {
        "id": "hujXI_g38zzz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_np = np.random.random((4,4))\n",
        "print(x_np)\n",
        "\n",
        "x_pt = torch.tensor(x_np)\n",
        "print(x_pt)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G6I5HeZ65oKx",
        "outputId": "a282c2e6-e536-4c02-982e-ef54e286e1a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.87052378 0.34696855 0.21667932 0.57079882]\n",
            " [0.02136561 0.85205241 0.84077682 0.71733416]\n",
            " [0.03388641 0.72535439 0.79655854 0.83792532]\n",
            " [0.15123618 0.99473453 0.9699564  0.06084524]]\n",
            "tensor([[0.8705, 0.3470, 0.2167, 0.5708],\n",
            "        [0.0214, 0.8521, 0.8408, 0.7173],\n",
            "        [0.0339, 0.7254, 0.7966, 0.8379],\n",
            "        [0.1512, 0.9947, 0.9700, 0.0608]], dtype=torch.float64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Both NumPy and torch support multiple different data types. By default, NumPy uses 64-bit floats, and PyTorch defaults to 32-bit floats. However, if you create a PyTorch tensor from a NumPy tensor, it uses the same type as the given NumPy tensor. You can see that in the previous output, where PyTorch let us know that dtype=torch.float64 since it is not the default choice."
      ],
      "metadata": {
        "id": "4uGmu0wN9KwK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The most common types we care about for deep learning are 32-bit floats, 64-bit integers (Longs), and booleans (i.e., binary True/False). Most operations leave the tensor type unchanged unless we explicitly create or cast it to a new type. To avoid issues with types, you can specify explicitly what type of tensor you want to create when calling a function. The following code checks what type of data is contained in our tensor using the dtype attribute:"
      ],
      "metadata": {
        "id": "KW4IsiOW9K2k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_np.dtype, x_pt.dtype)\n",
        "x_np = np.asarray(x_np, dtype=np.float32)\n",
        "x_pt = torch.tensor(x_np, dtype=torch.float32)\n",
        "print(x_np.dtype, x_pt.dtype)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fi1FZR9h9S0d",
        "outputId": "a4bfc785-9258-49c3-dfff-0ad1a149e4b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "float64 torch.float64\n",
            "float32 torch.float32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The main exception to using 32-bit floats or 64-bit integers as the dtype is when we need to perform logic operations (like Boolean AND, OR, NOT), which we can use to quickly create binary masks.\n",
        "A mask is a tensor that tells us which portions of another tensor are valid to use. We use masks in some of our more complex neural networks. For example, let’s say we want to find every value greater than 0.5 in a tensor. Both PyTorch and NumPy let us use the standard logic operators to check for things like this:"
      ],
      "metadata": {
        "id": "vNYAkMF1CSQU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "b_np = (x_np > 0.5)\n",
        "print(b_np)\n",
        "print(b_np.dtype)\n",
        "\n",
        "b_pt = (x_pt > 0.5)\n",
        "print(b_pt)\n",
        "print(b_pt.dtype)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yvi7Dinc_MT7",
        "outputId": "de8c5141-8e4b-4bf0-b65d-955007a50dce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ True False False  True]\n",
            " [False  True  True  True]\n",
            " [False  True  True  True]\n",
            " [False  True  True False]]\n",
            "bool\n",
            "tensor([[ True, False, False,  True],\n",
            "        [False,  True,  True,  True],\n",
            "        [False,  True,  True,  True],\n",
            "        [False,  True,  True, False]])\n",
            "torch.bool\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "While the NumPy and PyTorch APIs are not identical, they share many functions with the same names, behaviors, and characteristics:\n"
      ],
      "metadata": {
        "id": "mMeApECzCums"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.sum(x_np),torch.sum(x_pt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2c-waIh2CsyR",
        "outputId": "0615c73e-0121-43fb-bd2f-5b68a83d0236"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(9.006996, tensor(9.0070))"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "While many functions are the same, some are not quite identical. There may be slight differences in behavior or in the arguments required. These discrepancies are usually because the PyTorch version has made changes that are particular to how these methods are used for neural network design and execution. Following is an example of the transpose function, where PyTorch requires us to specify which two dimensions to transpose. NumPy takes the two dimensions and transposes them without complaint:"
      ],
      "metadata": {
        "id": "2HCxdHZ5C2UM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.transpose(x_np)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MhqptQPJCw-C",
        "outputId": "48c2ad11-af4a-432e-e195-7fe7f547892a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.87052375, 0.02136561, 0.03388641, 0.15123619],\n",
              "       [0.34696856, 0.8520524 , 0.7253544 , 0.9947345 ],\n",
              "       [0.21667932, 0.8407768 , 0.79655856, 0.9699564 ],\n",
              "       [0.5707988 , 0.71733415, 0.8379253 , 0.06084524]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.transpose(x_pt, 0, 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "khS3eykdDGMY",
        "outputId": "a879dfec-a1c8-4215-d3b4-d33c607c5557"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.8705, 0.0214, 0.0339, 0.1512],\n",
              "        [0.3470, 0.8521, 0.7254, 0.9947],\n",
              "        [0.2167, 0.8408, 0.7966, 0.9700],\n",
              "        [0.5708, 0.7173, 0.8379, 0.0608]])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PyTorch does this because we often want to transpose dimensions of a tensor for deep learning applications, whereas NumPy tries to stay with more general expectations. As shown next, we can transpose two of the dimensions in our torch_tensor3d from the start of the chapter. Originally it had a shape of (4, 2, 3). If we transpose the first and third dimensions, we get a shape of (3, 2, 4):"
      ],
      "metadata": {
        "id": "qhO8kjLVERAz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch_tensor3d\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1kFJjOoLDHsP",
        "outputId": "4186a00e-2a4d-4d80-c582-041b8b04f7b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 1,  2,  3],\n",
              "         [ 4,  5,  6]],\n",
              "\n",
              "        [[ 7,  8,  9],\n",
              "         [10, 11, 12]],\n",
              "\n",
              "        [[13, 14, 15],\n",
              "         [16, 17, 18]],\n",
              "\n",
              "        [[19, 20, 21],\n",
              "         [22, 23, 24]]])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.transpose(torch_tensor3d, 0, 2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ugO36jFQEMTZ",
        "outputId": "fdc69ec7-a0d0-41e6-8a6f-b0dd37fe722f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 1,  7, 13, 19],\n",
              "         [ 4, 10, 16, 22]],\n",
              "\n",
              "        [[ 2,  8, 14, 20],\n",
              "         [ 5, 11, 17, 23]],\n",
              "\n",
              "        [[ 3,  9, 15, 21],\n",
              "         [ 6, 12, 18, 24]]])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch.transpose(torch_tensor3d, 0, 2).shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8e0K6SxHEYvP",
        "outputId": "4917cfdd-1367-448a-8a4b-79a7517ba747"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 2, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Because such differences exist, you should always double-check the PyTorch documentation at https://pytorch.org/docs/stable/index.html if you attempt to use a function you are familiar with but suddenly find it does not behave as expected. It is also a good tool to have open when using PyTorch. There are many different functions that can help you in PyTorch, and we cannot review them all."
      ],
      "metadata": {
        "id": "PHleU2FZDTsD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PyTorch GPU acceleration\n",
        "\n",
        "We can use the timeit library: it lets us run code multiple times and tells us how long it took to run. We make a larger matrix X, compute XX several times, and see how long that takes to run:"
      ],
      "metadata": {
        "id": "es1_Dn1cDTw8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import timeit\n",
        "x = torch.rand(2**11, 2**11)\n",
        "time_cpu = timeit.timeit('x@x', globals=globals(), number=100)\n",
        "time_cpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xd0qPBUrEpnd",
        "outputId": "fd8ef787-14a1-4a0a-cfd3-3a334a593aa3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "23.691677253999984"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It takes a bit of time to run that code, but not too long. On my computer, it took 28.99 seconds to run, which is stored in the time_cpu variable. Now, how do we get PyTorch to use our GPU? First we need to create a device reference. We can ask PyTorch to give us one using the torch.device function. If you have an NVIDIA GPU, and the CUDA drivers are installed properly, you should be able to pass in \"cuda\" as a string and get back an object representing that device:"
      ],
      "metadata": {
        "id": "IJ6ly85kEZCu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Is CUDA available? :', torch.cuda.is_available())\n",
        "device = torch.device('cuda')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3cQP_WNFEZMd",
        "outputId": "e07c09db-b98c-4f38-cd52-8437a9ade26a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Is CUDA available? : True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "A6krHRQdEZUQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = x.to(device)\n",
        "time_gpu = timeit.timeit('x@x', globals=globals(), number=100)\n",
        "time_gpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V7S8yEiHEZc4",
        "outputId": "c2fb2d21-4489-4678-ebda-ab19eeb52b01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.807074027999988"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "When I run this code, the time to perform 100 multiplications is 2.8 seconds, which is an instant 8.5× speedup. This was a pretty ideal case, as matrix multiplications are super-efficient on GPUs, and we created a big matrix. You should try making the matrix smaller and see how that impacts the speedup you get."
      ],
      "metadata": {
        "id": "TSqO24heEZk4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Be aware that this only works if every object involved is on the same device. Say you run the following code, where the variable x has been moved onto the GPU and y has not (so it is on the CPU by default):"
      ],
      "metadata": {
        "id": "Jwc4roFCFNpy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.rand(128, 128).to(device)\n",
        "y = torch.rand(128, 128)\n",
        "x*y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "id": "xSg5RNedFQzD",
        "outputId": "383bb36c-11aa-4d05-f992-9c6d69fc3406"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-153fa9845a51>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mx\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You will end up getting an error message that says:\n",
        "RuntimeError: expected device cuda:0 but got device cpu\n",
        "The error tells you which device the first variable is on (cuda:0) but that the second variable was on a different device (cpu). If we instead wrote y*x you would see the error changetoexpected device cpu but got device cuda:0.Wheneveryouseeanerror like this, you have a bug that kept you from moving everything to the same compute device."
      ],
      "metadata": {
        "id": "73VGPufQFclC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The other thing to be aware of is how to convert PyTorch data back to the CPU. For example, we may want to convert a tensor back to a NumPy array so that we can pass it to Matplotlib or save it to disk. The PyTorch tensor object has a .numpy() method that will do this, but if you call x.numpy(), you will get this error:"
      ],
      "metadata": {
        "id": "58cRuQ_0Fd25"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x.numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "KJmojWLzFm_L",
        "outputId": "6c970260-703e-43fd-903b-2350098ee3a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-2527552080a3>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "-JEyDBNOFr1a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instead, you can use the handy shortcut function .cpu() to move an object back to the CPU, where you can interact with it normally. So you will often see code that looks like x.cpu().numpy() when you want to access the results of your work."
      ],
      "metadata": {
        "id": "6Q849wdCFsB7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "-Fp3X59DFyUw"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mOIf-DS6FoJ_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}